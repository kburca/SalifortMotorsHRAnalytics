{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span style=\"color:#0c0c0c\"><strong>SALIFORT MOTORS: HR ANALYTICS</strong></span>\n\n### <span style=\"color:#808080\"><strong>Case Study</strong></span>","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle"}},{"cell_type":"markdown","source":"Salifort Motors, a fictional French-based alternative energy vehicle manufacturer, aims to analyze their HR data and come up with insights and recommendations to improve employee retention.\n\nFor this case study, the PACE (Plan - Analyze - Construct - Execute) framework will be utilized.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#4784f0\"><strong>PLAN STAGE</strong></span>\n\n## <span style=\"color:#808080\">Understanding the business scenario and problem</span>\n\nThe HR department at Salifort Motors wants to take some initiatives to improve employee satisfaction levels at the company. They collected data from employees and is need of a data analytics professional that will provide data-driven suggestions based on the data. They have the following question: whatâ€™s likely to make the employee leave the company?\n\nThis project will focus on analyzing the data collected by the HR department and on building a model that will predict whether a employee will leave or stay in the company.\n\nThe company will also benefit by identifying factors that affect their decision to leave. It is time-consuming and expensive to find, interview, and hire new employees; thus, increasing employee retention will be advantageous to the company.\n\n\n## <span style=\"color:#808080\">Familiarizing with the HR dataset</span>\n\nThe dataset used in this case study is located on [Kaggle](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv), and it contains 15,000 rows and 10 columns for the variables listed below. \n\nVariable  |Description |\n-----|-----|\nsatisfaction_level|Employee-reported job satisfaction level [0&ndash;1]|\nlast_evaluation|Score of employee's last performance review [0&ndash;1]|\nnumber_project|Number of projects employee contributes to|\naverage_monthly_hours|Average number of hours employee worked per month|\ntime_spend_company|How long the employee has been with the company (years)\nWork_accident|Whether or not the employee experienced an accident while at work\nleft|Whether or not the employee left the company\npromotion_last_5years|Whether or not the employee was promoted in the last 5 years\nDepartment|The employee's department\nsalary|The employee's salary (U.S. dollars)","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Import Packages</span>","metadata":{}},{"cell_type":"code","source":"# Import packages\n### YOUR CODE HERE ### \n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For displaying all of the columns in dataframes\npd.set_option('display.max_columns', None)\n\n# For data modeling\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# For metrics and helpful functions\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,\\\nf1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.tree import plot_tree\n\n# For saving models\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:39:36.634477Z","iopub.execute_input":"2023-09-12T12:39:36.634982Z","iopub.status.idle":"2023-09-12T12:39:40.043684Z","shell.execute_reply.started":"2023-09-12T12:39:36.634951Z","shell.execute_reply":"2023-09-12T12:39:40.042145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Load Dataset</span>","metadata":{}},{"cell_type":"code","source":"df0 = pd.read_csv(\"/kaggle/input/hr-analytics-and-job-prediction/HR_comma_sep.csv\")\n\ndf0.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:40:25.267301Z","iopub.execute_input":"2023-09-12T12:40:25.267907Z","iopub.status.idle":"2023-09-12T12:40:25.356436Z","shell.execute_reply.started":"2023-09-12T12:40:25.267868Z","shell.execute_reply":"2023-09-12T12:40:25.354771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Initial Exploratory Data Analysis (EDA)</span>","metadata":{}},{"cell_type":"markdown","source":"Gather basic information about the data.","metadata":{}},{"cell_type":"code","source":"df0.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:40:29.617102Z","iopub.execute_input":"2023-09-12T12:40:29.617549Z","iopub.status.idle":"2023-09-12T12:40:29.658118Z","shell.execute_reply.started":"2023-09-12T12:40:29.617513Z","shell.execute_reply":"2023-09-12T12:40:29.656702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gather descriptive statistics about the data.","metadata":{}},{"cell_type":"code","source":"df0.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T21:42:11.109666Z","iopub.execute_input":"2023-09-11T21:42:11.110056Z","iopub.status.idle":"2023-09-11T21:42:11.166999Z","shell.execute_reply.started":"2023-09-11T21:42:11.110023Z","shell.execute_reply":"2023-09-11T21:42:11.165943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a data cleaning step, the columns were renamed and standardized so that they are all concise, correctly spelled and in `snake_case`.","metadata":{}},{"cell_type":"code","source":"df0 = df0.rename(columns={'Work_accident': 'work_accident',\n                          'average_montly_hours': 'average_monthly_hours',\n                          'time_spend_company': 'tenure',\n                          'Department': 'department'})\ndf0.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:41:31.594498Z","iopub.execute_input":"2023-09-12T12:41:31.594960Z","iopub.status.idle":"2023-09-12T12:41:31.607392Z","shell.execute_reply.started":"2023-09-12T12:41:31.594920Z","shell.execute_reply":"2023-09-12T12:41:31.606483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for missing values in the dataset.","metadata":{}},{"cell_type":"code","source":"df0.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:41:34.047748Z","iopub.execute_input":"2023-09-12T12:41:34.048123Z","iopub.status.idle":"2023-09-12T12:41:34.059741Z","shell.execute_reply.started":"2023-09-12T12:41:34.048093Z","shell.execute_reply":"2023-09-12T12:41:34.058764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset has no missing values.\n\n\nNext, check for duplicate entries in the data.","metadata":{}},{"cell_type":"code","source":"df0.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:41:36.579317Z","iopub.execute_input":"2023-09-12T12:41:36.580268Z","iopub.status.idle":"2023-09-12T12:41:36.598998Z","shell.execute_reply.started":"2023-09-12T12:41:36.580215Z","shell.execute_reply":"2023-09-12T12:41:36.596373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 3,008 rows that contain duplicates, which is about 20% of the data. Inspect some of the rows containing duplicates.","metadata":{}},{"cell_type":"code","source":"df0[df0.duplicated()].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:41:43.112382Z","iopub.execute_input":"2023-09-12T12:41:43.113116Z","iopub.status.idle":"2023-09-12T12:41:43.133190Z","shell.execute_reply.started":"2023-09-12T12:41:43.113075Z","shell.execute_reply":"2023-09-12T12:41:43.132083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With 10 columns and several continuous variables among them, it is likely that these observations are invalid so they will be dropped.","metadata":{}},{"cell_type":"code","source":"df1 = df0.drop_duplicates(keep='first')\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:41:48.120165Z","iopub.execute_input":"2023-09-12T12:41:48.120473Z","iopub.status.idle":"2023-09-12T12:41:48.146447Z","shell.execute_reply.started":"2023-09-12T12:41:48.120450Z","shell.execute_reply":"2023-09-12T12:41:48.145483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for outliers in the data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,3))\nplt.title('Boxplot to detect outliers for tenure', fontsize=10)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nsns.boxplot(x=df1['tenure'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:42:48.799591Z","iopub.execute_input":"2023-09-12T12:42:48.801098Z","iopub.status.idle":"2023-09-12T12:42:48.955610Z","shell.execute_reply.started":"2023-09-12T12:42:48.801056Z","shell.execute_reply":"2023-09-12T12:42:48.953658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The diagram shows that there are outliers in the `tenure` variable. Next, we will investigate the outliers in this particular column.","metadata":{}},{"cell_type":"code","source":"percentile25 = df1['tenure'].quantile(0.25)\npercentile75 = df1['tenure'].quantile(0.75)\niqr = percentile75 - percentile25\n\nupper_limit = percentile75 + 1.5 * iqr\nlower_limit = percentile25 - 1.5 * iqr\nprint(\"Lower limit:\", lower_limit)\nprint(\"Upper limit:\", upper_limit)\n\noutliers = df1[(df1['tenure'] > upper_limit) | (df1['tenure'] < lower_limit)]\n\nprint(\"Number of rows in the data with outliers in `tenure`:\", len(outliers))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:43:28.430849Z","iopub.execute_input":"2023-09-12T12:43:28.431224Z","iopub.status.idle":"2023-09-12T12:43:28.445867Z","shell.execute_reply.started":"2023-09-12T12:43:28.431194Z","shell.execute_reply":"2023-09-12T12:43:28.444659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some model types are susceptible to outliers. When we get to the stage of constructing the model, we will consider whether to remove or keep these outliers based on the type of model we decide to use.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#ec4434\"><strong>ANALYZE STAGE</strong></span>\n\n## <span style=\"color:#808080\">Exploratory Data Analysis (EDA)</span>\n\nCalculate how many employees left and what percentage of all employees this figure represents.","metadata":{}},{"cell_type":"code","source":"print(df1['left'].value_counts())\nprint()\n\nprint('Percentages:')\nprint((df1['left'].value_counts(normalize=True))*100)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T21:42:11.547363Z","iopub.execute_input":"2023-09-11T21:42:11.548422Z","iopub.status.idle":"2023-09-11T21:42:11.564985Z","shell.execute_reply.started":"2023-09-11T21:42:11.548385Z","shell.execute_reply":"2023-09-11T21:42:11.563900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an imbalance in the classes but the majority class does not go beyond 90%. Since the imbalance is not extreme, we will continue without modifying the class balance.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Data Visualizations</span>\nWe will create visualizations to examine the variables and their relationships to other variables variables in the dataset.\n\nLet us start looking at the `average_monthly_hours` and `number_project`.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (22,8))\n\n# Create boxplot showing `average_monthly_hours` distributions for `number_project`, comparing employees who stayed versus those who left\nsns.boxplot(data=df1, x='average_monthly_hours', y='number_project', hue='left', orient=\"h\", ax=ax[0])\nax[0].invert_yaxis()\nax[0].set_title('Monthly hours by number of projects', fontsize='14')\n\n# Create histogram showing distribution of `number_project`, comparing employees who stayed versus those who left\ntenure_stay = df1[df1['left']==0]['number_project']\ntenure_left = df1[df1['left']==1]['number_project']\nsns.histplot(data=df1, x='number_project', hue='left', multiple='dodge', shrink=2, ax=ax[1])\nax[1].set_title('Number of projects histogram', fontsize='14')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:44:14.082151Z","iopub.execute_input":"2023-09-12T12:44:14.082705Z","iopub.status.idle":"2023-09-12T12:44:14.867518Z","shell.execute_reply.started":"2023-09-12T12:44:14.082669Z","shell.execute_reply":"2023-09-12T12:44:14.866415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean hours of each group (those who stayed and those who left) increases with the number of projects worked. Naturally, people who are working on more projects would also be working for longer hours. However, a other things are notable from the visualizations.\n\n1. We see two prominent groups of employees who exited the company: (A) those whose average monthly hours are less than their peers, and (B) those who worked much more hours than their peers. Group A possibly includes those who were fired or employees who had already given their notice and were assigned fewer tasks because they were already on their way out of the company. For those in group B, it is reasonable to infer that these are the ones that quit. The employees in group B might have been the largest contributors to their projects. \n\n2. All employees who were assigned with seven projects left the company, and the interquartile ranges of this group and those who left with six projects was about 255 to 295 hours/week, which is much more than any other group. \n\n3. The optimal number of projects for employees to work on seems to be 3&ndash;4 since the ratio of left/stayed is very small for these groups.\n\n4. Assuming a work week of 40 hours and two weeks of vacation per year, the average number of working hours per month of employees working Monday&ndash;Friday `= 50 weeks * 40 hours per week / 12 months = 166.7 hours per month`. This means that, aside from the employees who worked on only two projects, every group&mdash;even those who did not leave the company&mdash;worked considerably more hours than this. It seems that employees in the company are overworked.\n\n5. In addition, the increase in mean hours for people working on more projects were much more evident for the people who left than those people who stayed. This indicates that the people who quitted were even much more overworked that those who did not.\n\nValidate and confirm that all employees with seven projects left.","metadata":{}},{"cell_type":"code","source":"df1[df1['number_project']==7]['left'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:45:07.618004Z","iopub.execute_input":"2023-09-12T12:45:07.618409Z","iopub.status.idle":"2023-09-12T12:45:07.628748Z","shell.execute_reply.started":"2023-09-12T12:45:07.618378Z","shell.execute_reply":"2023-09-12T12:45:07.627691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This confirms that all employees with 7 projects did leave. \n\nNext, examine the average monthly hours versus the satisfaction levels. ","metadata":{}},{"cell_type":"code","source":"# Create scatterplot of `average_monthly_hours` versus `satisfaction_level`, \n# comparing employees who stayed versus those who left\nplt.figure(figsize=(16, 9))\nsns.scatterplot(data=df1, x='average_monthly_hours', y='satisfaction_level', hue='left', alpha=0.4)\nplt.axvline(x=166.7, color='#ff6361', label='166.7 hrs./mo.', ls='--')\nplt.legend(labels=['166.67 hrs./mo.', 'stayed', 'left'])\nplt.title('Monthly hours by last evaluation score', fontsize='12')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:45:45.796458Z","iopub.execute_input":"2023-09-12T12:45:45.797882Z","iopub.status.idle":"2023-09-12T12:45:46.817058Z","shell.execute_reply.started":"2023-09-12T12:45:45.797805Z","shell.execute_reply":"2023-09-12T12:45:46.815301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The diagram shows that there is a significant number of employees who worked about 240&ndash;315 hours per month, or over 75 hours per week for a whole year. Most likely, this is related to their satisfaction levels being close to zero. Secondly, the plot shows another group of people who left, those who had more normal working hours. Even so, their satisfaction was only around 0.4. It is possible they felt pressured to work more, given that many of their peers are working more hours. There is another group who worked about 210&ndash;280 hours per month, and they had satisfaction levels ranging about 0.7&ndash;0.9. \n\nThe strange shape of the distributions in the dataset is indicative of data manipulation or synthetic data.\n\nFor the next visualizations, it might be interesting to visualize satisfaction levels and tenure.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(4,3))\n# Create histogram showing distribution of `satisfaction_level`\ntenure_left = df1[df1['left']==1]['satisfaction_level']\nsns.histplot(data=tenure_left, color ='#e1812b')\nplt.title('Satisfaction histogram for those who left', fontsize='10')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:46:01.436709Z","iopub.execute_input":"2023-09-12T12:46:01.437140Z","iopub.status.idle":"2023-09-12T12:46:01.678263Z","shell.execute_reply.started":"2023-09-12T12:46:01.437106Z","shell.execute_reply":"2023-09-12T12:46:01.676443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (22,8))\n\n# Create boxplot showing distributions of `satisfaction_level` by tenure, comparing employees who stayed versus those who left\nsns.boxplot(data=df1, x='satisfaction_level', y='tenure', hue='left', orient=\"h\", ax=ax[0])\nax[0].invert_yaxis()\nax[0].set_title('Satisfaction by Tenure', fontsize='14')\n\n# Create histogram showing distribution of `tenure`, comparing employees who stayed versus those who left\ntenure_stay = df1[df1['left']==0]['tenure']\ntenure_left = df1[df1['left']==1]['tenure']\nsns.histplot(data=df1, x='tenure', hue='left', multiple='dodge', shrink=5, ax=ax[1])\nax[1].set_title('Tenure Histogram', fontsize='14')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:47:00.746025Z","iopub.execute_input":"2023-09-12T12:47:00.746444Z","iopub.status.idle":"2023-09-12T12:47:01.775652Z","shell.execute_reply.started":"2023-09-12T12:47:00.746411Z","shell.execute_reply":"2023-09-12T12:47:01.774230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notable observations based from the visualizations:\n\n- There are two general categories for employees who left: (A) dissatisfied employees with short tenures and, (B) very satisfied employees with medium-length tenures.\n- Employees who left with tenure of 4 years have an unusually low satisfaction level. It is worth investigating company policies and changes in these policies that might have affected employees. \n- Employees with more than 7 years tenure did not leave and their satisfaction levels were also high. \n- There are relatively few longer-tenured employees. It is possible that they are the higher-ranking and higher-paid employees.\n\nNext, calculate the mean and median satisfaction scores of employees who left and those who did not.","metadata":{}},{"cell_type":"code","source":"df1.groupby(['left'])['satisfaction_level'].agg(['mean','median'])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:47:44.439865Z","iopub.execute_input":"2023-09-12T12:47:44.440286Z","iopub.status.idle":"2023-09-12T12:47:44.458505Z","shell.execute_reply.started":"2023-09-12T12:47:44.440256Z","shell.execute_reply":"2023-09-12T12:47:44.456758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean and median satisfaction scores of employees who left are lower than those of employees who stayed, which is as expected. Among the employees who stayed, the mean satisfaction score is slightly below the median score. This indicates a left-skewed satisfaction level distribution\n\nNext, examine salary levels for different tenures.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (22,8))\n\n# Define short-tenured employees\ntenure_short = df1[df1['tenure']<6]\n\n# Define long-tenured employees\ntenure_long = df1[df1['tenure']>6]\n\n# Plot short-tenured histogram\nsns.histplot(data=tenure_short, x='tenure', hue='salary', discrete=1, \n             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.5, ax=ax[0])\nax[0].set_title('Salary Histogram by Tenure: Short-tenured', fontsize='14')\n\n# Plot long-tenured histogram\nsns.histplot(data=tenure_long, x='tenure', hue='salary', discrete=1, \n             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.4, ax=ax[1])\nax[1].set_title('Salary Histogram by Tenure: Long-tenured', fontsize='14');","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:49:35.028087Z","iopub.execute_input":"2023-09-12T12:49:35.028514Z","iopub.status.idle":"2023-09-12T12:49:35.762897Z","shell.execute_reply.started":"2023-09-12T12:49:35.028473Z","shell.execute_reply":"2023-09-12T12:49:35.761092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Long-tenured employees were not majorly comprised of higher-paid employees. \n\nNext, explore whether there is a correlation between working long hours and receiving high evaluation scores. Create a scatterplot of `average_monthly_hours` versus `last_evaluation`.","metadata":{}},{"cell_type":"code","source":"# Create scatterplot of `average_monthly_hours` versus `last_evaluation`\nplt.figure(figsize=(16, 9))\nsns.scatterplot(data=df1, x='average_monthly_hours', y='last_evaluation', hue='left', alpha=0.4)\nplt.axvline(x=166.7, color='#ff6361', label='166.7 hrs./mo.', ls='--')\nplt.legend(labels=['166.7 hrs./mo.', 'left', 'stayed'])\nplt.title('Monthly Hours by Last Evaluation Score', fontsize='12')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:50:29.473333Z","iopub.execute_input":"2023-09-12T12:50:29.473817Z","iopub.status.idle":"2023-09-12T12:50:30.620593Z","shell.execute_reply.started":"2023-09-12T12:50:29.473785Z","shell.execute_reply":"2023-09-12T12:50:30.618787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notable observations:\n- There are two groups of employees who left: well-performing employees who are overworked and low-performing employees who worked slightly below 167 hours. \n- Working long hours does not guarantee a good evaluation score.\n- Most of the employees in this company work well over 167 hours per month.\n\nNext, examine whether employees who worked very long hours were promoted in the last five years.","metadata":{}},{"cell_type":"code","source":"# Create plot to examine relationship between `average_monthly_hours` and `promotion_last_5years`\nplt.figure(figsize=(16, 3))\nsns.scatterplot(data=df1, x='average_monthly_hours', y='promotion_last_5years', hue='left', alpha=0.4)\nplt.axvline(x=166.7, color='#ff6361', ls='--')\nplt.legend(labels=['166.7 hrs./mo.', 'left', 'stayed'])\nplt.title('Monthly Hours by Promotion in the Last 5 years', fontsize='12')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:51:23.385490Z","iopub.execute_input":"2023-09-12T12:51:23.386347Z","iopub.status.idle":"2023-09-12T12:51:24.324926Z","shell.execute_reply.started":"2023-09-12T12:51:23.386319Z","shell.execute_reply":"2023-09-12T12:51:24.323392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations from the plot:\n- the employees who left were working the longest hours\n- only few employees who were promoted in the last five years left\n- only few employees who worked the most hours were promoted\n\nNext step is to inspect how the employees who left are distributed across departments.","metadata":{}},{"cell_type":"code","source":"df1[\"department\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:51:27.802936Z","iopub.execute_input":"2023-09-12T12:51:27.803355Z","iopub.status.idle":"2023-09-12T12:51:27.816163Z","shell.execute_reply.started":"2023-09-12T12:51:27.803313Z","shell.execute_reply":"2023-09-12T12:51:27.813979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create stacked histogram to compare department distribution of employees who left to that of employees who didn't\nplt.figure(figsize=(11,8))\nsns.histplot(data=df1, x='department', hue='left', discrete=1, \n             hue_order=[0, 1], multiple='dodge', shrink=.5)\nplt.xticks(rotation=45)\nplt.title('Counts of Stayed/Left by Department', fontsize=12)\nplt.ylabel('Employee Count')\nplt.xlabel('Department')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:18:35.696002Z","iopub.execute_input":"2023-09-12T13:18:35.696365Z","iopub.status.idle":"2023-09-12T13:18:36.040913Z","shell.execute_reply.started":"2023-09-12T13:18:35.696337Z","shell.execute_reply":"2023-09-12T13:18:36.039944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The proportion of employees who left to those who stayed are similar to all departments. \n\nLastly, check for correlations between variables in the data.","metadata":{}},{"cell_type":"code","source":"# Plot a correlation heatmap\nplt.figure(figsize=(16, 9))\nheatmap = sns.heatmap(df0[['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure', 'work_accident', 'left','promotion_last_5years']].corr(), vmin=-1, vmax=1, annot=True, cmap=sns.color_palette(\"YlGnBu\", as_cmap=True))\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':14}, pad=12)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:06:35.296846Z","iopub.execute_input":"2023-09-12T13:06:35.297266Z","iopub.status.idle":"2023-09-12T13:06:35.880878Z","shell.execute_reply.started":"2023-09-12T13:06:35.297233Z","shell.execute_reply":"2023-09-12T13:06:35.879141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based from the correlation heatmap, the number of projects, monthly hours, and evaluation scores have some positive correlation with each other. Satisfaction level is negatively correlated with the tendency of an employee leaving.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Insights</span>\n\nIt can be inferred that **poor management** causes the employees to leave the company. Leaving is correlated to **working longer hours**, **more assigned projects**, and **lower satisfaction levels**. This is expected because it can be disappointing to work long hours and not receive any promotion or good evaluation scores. There is a significant number of employees at this company who are most likely burnt out. It also appears that beyond six years at the company, employees tend to stay. ","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#f3a909\"><strong>CONSTRUCT STAGE</strong></span>\n\n## <span style=\"color:#808080\">Identify Type of Prediction Task</span>\n\nThe goal is to predict whether an employee leaves the company, which is a categorical outcome variable. So this task involves classification. More specifically, this involves binary classification, since the outcome variable `left` can be either 1 (indicating employee left) or 0 (indicating employee did not leave). \n\nSince the variable to be predicted is categorical, a Logistic Regression model or a Tree-based Machine Learning model is appropriate for this project.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Logistic Regression Model</span>","metadata":{}},{"cell_type":"markdown","source":"First, encode the two non-numeric variables: `department` and `salary`. The variable `salary` is ordinal, meaning, there is a heirarchy to the categories; therefore, it is better to convert the levels to numeric values, 0&ndash;2.","metadata":{}},{"cell_type":"code","source":"df1['salary'].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:11:28.063357Z","iopub.execute_input":"2023-09-12T13:11:28.063777Z","iopub.status.idle":"2023-09-12T13:11:28.073348Z","shell.execute_reply.started":"2023-09-12T13:11:28.063748Z","shell.execute_reply":"2023-09-12T13:11:28.071948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_enc = df1.copy()\n\n# Dummy encode the `department` column\ndf_enc = pd.get_dummies(df_enc, drop_first=False, columns=['department'])\n\n# Encode the `salary` column as an ordinal numeric category\ndf_enc['salary'] = (df_enc['salary'].astype('category').cat.set_categories(['low', 'medium', 'high']).cat.codes)\n\ndf_enc.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:11:36.780173Z","iopub.execute_input":"2023-09-12T13:11:36.780675Z","iopub.status.idle":"2023-09-12T13:11:36.814887Z","shell.execute_reply.started":"2023-09-12T13:11:36.780610Z","shell.execute_reply":"2023-09-12T13:11:36.813869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a heatmap to visualize how correlated variables are\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_enc[['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure']]\n            .corr(), annot=True, cmap=sns.color_palette(\"YlGnBu\", as_cmap=True))\nplt.title('Heatmap of the Dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:13:47.428864Z","iopub.execute_input":"2023-09-12T13:13:47.429296Z","iopub.status.idle":"2023-09-12T13:13:47.744220Z","shell.execute_reply.started":"2023-09-12T13:13:47.429263Z","shell.execute_reply":"2023-09-12T13:13:47.742851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a stacked bart plot to visualize number of employees across department, comparing those who left with those who didn't\n# In the legend, 0 (purple color) represents employees who did not leave, 1 (red color) represents employees who left\npd.crosstab(df1['department'], df1['left']).plot(kind ='bar',color='cr')\nplt.title('Counts of Employees Who Left Versus Stayed Across Each Department', fontsize=12)\nplt.ylabel('Employee Count')\nplt.xlabel('Department')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:34.453557Z","iopub.execute_input":"2023-09-12T13:17:34.454007Z","iopub.status.idle":"2023-09-12T13:17:34.721504Z","shell.execute_reply.started":"2023-09-12T13:17:34.453970Z","shell.execute_reply":"2023-09-12T13:17:34.719501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic regression models are susceptible to outliers; therefore, it is recommended to remove the outliers in the `tenure` column, as identified in the Plan Stage.","metadata":{}},{"cell_type":"code","source":"# Select rows without outliers in `tenure` and save resulting dataframe in a new variable\ndf_logreg = df_enc[(df_enc['tenure'] >= lower_limit) & (df_enc['tenure'] <= upper_limit)]\n\ndf_logreg.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:19:06.619813Z","iopub.execute_input":"2023-09-12T13:19:06.620240Z","iopub.status.idle":"2023-09-12T13:19:06.647076Z","shell.execute_reply.started":"2023-09-12T13:19:06.620201Z","shell.execute_reply":"2023-09-12T13:19:06.645839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, isolate the outcome variable as `y` and select the features `X` to be used in the model.","metadata":{}},{"cell_type":"code","source":"# Isolate the outcome variable\ny = df_logreg['left']\n\n# Select the features you want to use in your model\nX = df_logreg.drop('left', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:19:20.214774Z","iopub.execute_input":"2023-09-12T13:19:20.215212Z","iopub.status.idle":"2023-09-12T13:19:20.222993Z","shell.execute_reply.started":"2023-09-12T13:19:20.215178Z","shell.execute_reply":"2023-09-12T13:19:20.221474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data into training set and testing set and since the classes are imbalanced, data is stratified based on the values of the outcome variable. Assign a value in the `random_state` for reproducibility.","metadata":{}},{"cell_type":"code","source":"# Split the data into training set and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:19:23.862736Z","iopub.execute_input":"2023-09-12T13:19:23.863227Z","iopub.status.idle":"2023-09-12T13:19:23.879443Z","shell.execute_reply.started":"2023-09-12T13:19:23.863191Z","shell.execute_reply":"2023-09-12T13:19:23.878137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, construct the logistic regression model and plot a confusion matrix to visualize the results of the model after it was used to make predictions using the test set.","metadata":{}},{"cell_type":"code","source":"# Construct a logistic regression model and fit it to the training dataset\nlog_clf = LogisticRegression(random_state=42, max_iter=500).fit(X_train, y_train)\n\n# Use the logistic regression model to get predictions on the test set\ny_pred = log_clf.predict(X_test)\n\n# Compute values for confusion matrix\nlog_cm = confusion_matrix(y_test, y_pred, labels=log_clf.classes_)\n\n# Create display of confusion matrix\nlog_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, \n                                  display_labels=log_clf.classes_)\n\n# Plot confusion matrix\nlog_disp.plot(values_format='', cmap=plt.cm.YlGnBu)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:40:20.681960Z","iopub.execute_input":"2023-09-12T13:40:20.682416Z","iopub.status.idle":"2023-09-12T13:40:21.290569Z","shell.execute_reply.started":"2023-09-12T13:40:20.682377Z","shell.execute_reply":"2023-09-12T13:40:21.289538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"True negatives: Employees who did not leave that the model correctly predicted; upper-left quadrant\n\nFalse positives: Employees who did not leave the model incorrectly predicted as leaving; upper-right quadrant\n\nFalse negatives: Employees who actually left that the model inaccurately predicted as staying; bottom-left quadrant\n\nTrue positives: Employees who left the company that the model accurately predicted; bottom-right quadrant\n\nA perfect model would yield all true negatives and true positives, and no false negatives or false positives.","metadata":{}},{"cell_type":"markdown","source":"To summarize the results and evaluate the performance of the logistic regression model in terms of the key metrics, create a classification report that includes precision, recall, f1-score, and accuracy.","metadata":{}},{"cell_type":"code","source":"# Create classification report for logistic regression model\ntarget_names = ['Predicted would not leave', 'Predicted would leave']\nprint(classification_report(y_test, y_pred, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T21:42:22.023200Z","iopub.execute_input":"2023-09-11T21:42:22.026022Z","iopub.status.idle":"2023-09-11T21:42:22.045846Z","shell.execute_reply.started":"2023-09-11T21:42:22.025981Z","shell.execute_reply":"2023-09-11T21:42:22.044269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classification report above shows that the Logistic Regression model was able to achieve a precision of 86%, recall of 93%, f1-score of 90% and accuracy of 82%.\n\nThe model performance in predicting employee retention is good; however, the logistic regression model assumes linearity, independent observations, and no multicollinearity. Also, recall that we removed outliers and the outliers might have been valid data points. Therefore, it is recommended to consider tree-based models, which do not have these assumptions and are not affected by extreme outliers.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Tree-based Model</span>\n\nThis modeling approach will cover implementation of Decision Tree, Random Forest, and XGBoost.\n\nFirstly, isolate the outcome variable as `y` and select the features `X` to be used in the model.","metadata":{}},{"cell_type":"code","source":"# Isolate the outcome variable\ny = df_logreg['left']\n\n# Select the features you want to use in your model\nX = df_logreg.drop('left', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:45:04.740454Z","iopub.execute_input":"2023-09-12T13:45:04.740894Z","iopub.status.idle":"2023-09-12T13:45:04.750487Z","shell.execute_reply.started":"2023-09-12T13:45:04.740860Z","shell.execute_reply":"2023-09-12T13:45:04.748863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data into training set and testing set and since the classes are imbalanced, data is stratified based on the values of the outcome variable. Assign a value in the `random_state` for reproducibility.","metadata":{}},{"cell_type":"code","source":"# Split the data into training set and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:45:20.087834Z","iopub.execute_input":"2023-09-12T13:45:20.088222Z","iopub.status.idle":"2023-09-12T13:45:20.105761Z","shell.execute_reply.started":"2023-09-12T13:45:20.088196Z","shell.execute_reply":"2023-09-12T13:45:20.103999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this project, we will use F1 as the metric to refit the models since the class is imbalanced and because of its easier interpretability, which in some cases can be a deciding factor for stakeholder decisions.","metadata":{}},{"cell_type":"markdown","source":"We will also pickle our fit models for saving and quicker access. Define a path for saving the model, and define functions to pickle the model and read in the model.","metadata":{}},{"cell_type":"code","source":"# Define a path to the folder where you want to save the model\npath = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:45:26.621359Z","iopub.execute_input":"2023-09-12T13:45:26.621890Z","iopub.status.idle":"2023-09-12T13:45:26.628390Z","shell.execute_reply.started":"2023-09-12T13:45:26.621855Z","shell.execute_reply":"2023-09-12T13:45:26.626839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_pickle(path, model_object, save_as:str):\n    '''\n    In: \n        path:         path of folder where you want to save the pickle\n        model_object: a model you want to pickle\n        save_as:      filename for how you want to save the model\n\n    Out: A call to pickle the model in the folder indicated\n    '''    \n\n    with open(path + save_as + '.pickle', 'wb') as to_write:\n        pickle.dump(model_object, to_write)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:45:30.585665Z","iopub.execute_input":"2023-09-12T13:45:30.586108Z","iopub.status.idle":"2023-09-12T13:45:30.595046Z","shell.execute_reply.started":"2023-09-12T13:45:30.586075Z","shell.execute_reply":"2023-09-12T13:45:30.592955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_pickle(path, saved_model_name:str):\n    '''\n    In: \n        path:             path to folder where you want to read from\n        saved_model_name: filename of pickled model you want to read in\n\n    Out: \n        model: the pickled model \n    '''\n    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n        model = pickle.load(to_read)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:45:34.402708Z","iopub.execute_input":"2023-09-12T13:45:34.403128Z","iopub.status.idle":"2023-09-12T13:45:34.410308Z","shell.execute_reply.started":"2023-09-12T13:45:34.403093Z","shell.execute_reply":"2023-09-12T13:45:34.408555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"color:#808080\"><strong>Round 1</strong></span>","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">Decision Tree</span>\n\nConstruct a Decision Tree and with Cross Validation, through GridSearch, to exhaustively search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# Instantiate model\ntree = DecisionTreeClassifier(random_state=0)\n\n# Assign a dictionary of hyperparameters to search over\ncv_params = {'max_depth':[4, 6, 8, None],\n             'min_samples_leaf': [2, 5, 1],\n             'min_samples_split': [2, 4, 6]\n             }\n\n# Assign a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# Instantiate GridSearch\ntree1 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='f1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:45:39.812477Z","iopub.execute_input":"2023-09-12T13:45:39.812938Z","iopub.status.idle":"2023-09-12T13:45:39.822850Z","shell.execute_reply.started":"2023-09-12T13:45:39.812903Z","shell.execute_reply":"2023-09-12T13:45:39.821081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the decision tree model to the training data.\ntree1.fit(X_train, y_train) #Wall time: ~5s","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:48:27.206348Z","iopub.execute_input":"2023-09-12T13:48:27.207475Z","iopub.status.idle":"2023-09-12T13:48:32.011351Z","shell.execute_reply.started":"2023-09-12T13:48:27.207423Z","shell.execute_reply":"2023-09-12T13:48:32.010361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write pickle\nwrite_pickle(path, tree1, 'hr_tree1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:49:21.008328Z","iopub.execute_input":"2023-09-12T13:49:21.009060Z","iopub.status.idle":"2023-09-12T13:49:21.016546Z","shell.execute_reply.started":"2023-09-12T13:49:21.008994Z","shell.execute_reply":"2023-09-12T13:49:21.014765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read pickle\ntree1 = read_pickle(path, 'hr_tree1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:49:25.321878Z","iopub.execute_input":"2023-09-12T13:49:25.322322Z","iopub.status.idle":"2023-09-12T13:49:25.329248Z","shell.execute_reply.started":"2023-09-12T13:49:25.322287Z","shell.execute_reply":"2023-09-12T13:49:25.328331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best parameters\ntree1.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:49:31.050025Z","iopub.execute_input":"2023-09-12T13:49:31.050484Z","iopub.status.idle":"2023-09-12T13:49:31.057983Z","shell.execute_reply.started":"2023-09-12T13:49:31.050447Z","shell.execute_reply":"2023-09-12T13:49:31.056314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best F1 score on CV\ntree1.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:49:34.053749Z","iopub.execute_input":"2023-09-12T13:49:34.054126Z","iopub.status.idle":"2023-09-12T13:49:34.064480Z","shell.execute_reply.started":"2023-09-12T13:49:34.054101Z","shell.execute_reply":"2023-09-12T13:49:34.063227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The F1 score is strong, indicating that the model can predict employees who will leave accurately.","metadata":{}},{"cell_type":"code","source":"def make_results(model_name:str, model_object, metric:str):\n    '''\n    Arguments:\n        model_name (string): what you want the model to be called in the output table\n        model_object: a fit GridSearchCV object\n        metric (string): precision, recall, f1, accuracy, or auc\n  \n    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores\n    for the model with the best mean 'metric' score across all validation folds.  \n    '''\n\n    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n    metric_dict = {'auc': 'mean_test_roc_auc',\n                   'precision': 'mean_test_precision',\n                   'recall': 'mean_test_recall',\n                   'f1': 'mean_test_f1',\n                   'accuracy': 'mean_test_accuracy'\n                  }\n\n    # Get all the results from the CV and put them in a df\n    cv_results = pd.DataFrame(model_object.cv_results_)\n\n    # Isolate the row of the df with the max(metric) score\n    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n\n    # Extract Accuracy, precision, recall, and f1 score from that row\n    auc = best_estimator_results.mean_test_roc_auc\n    f1 = best_estimator_results.mean_test_f1\n    recall = best_estimator_results.mean_test_recall\n    precision = best_estimator_results.mean_test_precision\n    accuracy = best_estimator_results.mean_test_accuracy\n  \n    # Create table of results\n    table = pd.DataFrame()\n    table = pd.DataFrame({'model': [model_name],\n                          'precision': [precision],\n                          'recall': [recall],\n                          'F1': [f1],\n                          'accuracy': [accuracy],\n                          'auc': [auc]\n                        })\n  \n    return table","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:49:40.290588Z","iopub.execute_input":"2023-09-12T13:49:40.291066Z","iopub.status.idle":"2023-09-12T13:49:40.298923Z","shell.execute_reply.started":"2023-09-12T13:49:40.291032Z","shell.execute_reply":"2023-09-12T13:49:40.297938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all CV scores\ntree_cv_results = []\ntree_cv_results = make_results('Decision Tree CV 1', tree1, 'f1')\ntree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:49:51.148931Z","iopub.execute_input":"2023-09-12T13:49:51.149374Z","iopub.status.idle":"2023-09-12T13:49:51.175558Z","shell.execute_reply.started":"2023-09-12T13:49:51.149341Z","shell.execute_reply":"2023-09-12T13:49:51.173079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Strong scores indicates good model performance. Decision trees can be vulnerable to overfitting; therefore, it is wise to construct a random forest for comparison.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">Random Forest</span>\n\nConstruct a Random Forest and with Cross Validation, through GridSearch, to exhaustively search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# Instantiate model\nrf = RandomForestClassifier(random_state=0)\n\n# Assign a dictionary of hyperparameters to search over\ncv_params = {'max_depth': [3,5, None], \n             'max_features': [1.0],\n             'max_samples': [0.7, 1.0],\n             'min_samples_leaf': [1,2,3],\n             'min_samples_split': [2,3,4],\n             'n_estimators': [300, 500],\n             }  \n\n# Assign a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# Instantiate GridSearch\nrf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='f1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:49:59.122599Z","iopub.execute_input":"2023-09-12T13:49:59.123031Z","iopub.status.idle":"2023-09-12T13:49:59.131893Z","shell.execute_reply.started":"2023-09-12T13:49:59.122999Z","shell.execute_reply":"2023-09-12T13:49:59.130059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the random forest model to the training data.\nrf1.fit(X_train, y_train) #Wall time: ~29min","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:50:24.331712Z","iopub.execute_input":"2023-09-12T13:50:24.332082Z","iopub.status.idle":"2023-09-12T14:19:42.452816Z","shell.execute_reply.started":"2023-09-12T13:50:24.332052Z","shell.execute_reply":"2023-09-12T14:19:42.450927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write pickle\nwrite_pickle(path, rf1, 'hr_rf1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:19:53.170870Z","iopub.execute_input":"2023-09-12T14:19:53.171294Z","iopub.status.idle":"2023-09-12T14:19:53.201309Z","shell.execute_reply.started":"2023-09-12T14:19:53.171262Z","shell.execute_reply":"2023-09-12T14:19:53.200066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read pickle\nrf1 = read_pickle(path, 'hr_rf1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:20:03.353683Z","iopub.execute_input":"2023-09-12T14:20:03.354129Z","iopub.status.idle":"2023-09-12T14:20:03.383569Z","shell.execute_reply.started":"2023-09-12T14:20:03.354096Z","shell.execute_reply":"2023-09-12T14:20:03.381231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best params\nrf1.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:20:14.434936Z","iopub.execute_input":"2023-09-12T14:20:14.435347Z","iopub.status.idle":"2023-09-12T14:20:14.441944Z","shell.execute_reply.started":"2023-09-12T14:20:14.435313Z","shell.execute_reply":"2023-09-12T14:20:14.441096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best F1 score on CV\nrf1.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:20:18.213610Z","iopub.execute_input":"2023-09-12T14:20:18.214336Z","iopub.status.idle":"2023-09-12T14:20:18.221742Z","shell.execute_reply.started":"2023-09-12T14:20:18.214278Z","shell.execute_reply":"2023-09-12T14:20:18.220049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all CV scores using the make_results() function defined earlier.\nrf1_cv_results = make_results('Random Forest CV 1', rf1, 'f1')\ntree_cv_results = pd.concat([tree_cv_results, rf1_cv_results], axis=0)\ntree_cv_results = tree_cv_results.reset_index(drop=True)\ntree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:20:23.376098Z","iopub.execute_input":"2023-09-12T14:20:23.377418Z","iopub.status.idle":"2023-09-12T14:20:23.405183Z","shell.execute_reply.started":"2023-09-12T14:20:23.377360Z","shell.execute_reply":"2023-09-12T14:20:23.403793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In case a row is to be dropped, input index number(s) in drop()\n#tree_cv_results = tree_cv_results.drop([5,6], axis=0).reset_index(drop=True)\n#tree_cv_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the evaluation scores, the random forest is generally a better model than the decision tree. \n\nNext, check if XGBoost is a better model than the two approaches implemented thus far.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">XGBoost</span>\n\nConstruct an XGBoost model and with Cross Validation, through GridSearch, to exhaustively search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# 1. Instantiate the XGBoost classifier\nxgb = XGBClassifier(objective='binary:logistic', random_state=0)\n\n# 2. Create a dictionary of hyperparameters to tune\ncv_params = {'learning_rate': [0.1,0.2,0.3],\n             'max_depth': [1,3,5,7],\n             'min_child_weight': [0.2,0.5,0.9,1,2,5],\n             'n_estimators': [30,50,100,200]\n             }\n\n# 3. Define a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# 4. Instantiate the GridSearchCV object\nxgb1 = GridSearchCV(xgb, cv_params, scoring=scoring, cv=4, refit='f1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:20:42.342422Z","iopub.execute_input":"2023-09-12T14:20:42.342822Z","iopub.status.idle":"2023-09-12T14:20:42.351284Z","shell.execute_reply.started":"2023-09-12T14:20:42.342792Z","shell.execute_reply":"2023-09-12T14:20:42.349509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the XGBoost model to the training data.\nxgb1.fit(X_train, y_train) #Wall time: ~5min","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:21:33.431288Z","iopub.execute_input":"2023-09-12T14:21:33.431686Z","iopub.status.idle":"2023-09-12T14:26:49.470525Z","shell.execute_reply.started":"2023-09-12T14:21:33.431648Z","shell.execute_reply":"2023-09-12T14:26:49.469730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write pickle\nwrite_pickle(path, xgb1, 'hr_xgb1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:27:04.971350Z","iopub.execute_input":"2023-09-12T14:27:04.971814Z","iopub.status.idle":"2023-09-12T14:27:04.986027Z","shell.execute_reply.started":"2023-09-12T14:27:04.971780Z","shell.execute_reply":"2023-09-12T14:27:04.984702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read pickle\nxgb1 = read_pickle(path, 'hr_xgb1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:27:10.042666Z","iopub.execute_input":"2023-09-12T14:27:10.043118Z","iopub.status.idle":"2023-09-12T14:27:10.055614Z","shell.execute_reply.started":"2023-09-12T14:27:10.043084Z","shell.execute_reply":"2023-09-12T14:27:10.054792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examine best parameters\nxgb1.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:27:27.396192Z","iopub.execute_input":"2023-09-12T14:27:27.396555Z","iopub.status.idle":"2023-09-12T14:27:27.404146Z","shell.execute_reply.started":"2023-09-12T14:27:27.396531Z","shell.execute_reply":"2023-09-12T14:27:27.403016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examine best score\nxgb1.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:27:30.751964Z","iopub.execute_input":"2023-09-12T14:27:30.752395Z","iopub.status.idle":"2023-09-12T14:27:30.759964Z","shell.execute_reply.started":"2023-09-12T14:27:30.752362Z","shell.execute_reply":"2023-09-12T14:27:30.758844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call 'make_results()' on the GridSearch object\nxgb1_cv_results = make_results('XGB CV 1', xgb1, 'f1')\ntree_cv_results = pd.concat([tree_cv_results, xgb1_cv_results], axis=0)\ntree_cv_results = tree_cv_results.reset_index(drop=True)\ntree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:27:34.469361Z","iopub.execute_input":"2023-09-12T14:27:34.469812Z","iopub.status.idle":"2023-09-12T14:27:34.490286Z","shell.execute_reply.started":"2023-09-12T14:27:34.469777Z","shell.execute_reply":"2023-09-12T14:27:34.489301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In case a row is to be dropped, input index number(s) in drop()\n#tree_cv_results = tree_cv_results.drop([5,6], axis=0).reset_index(drop=True)\n#tree_cv_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that the Random Forest model outperforms Decision Tree and XGBoost.","metadata":{}},{"cell_type":"markdown","source":"Define a function that will get all the scores of a model's performance on the test set.","metadata":{}},{"cell_type":"code","source":"def get_scores(model_name:str, model, X_test_data, y_test_data):\n    '''\n    Generate a table of test scores.\n\n    In: \n        model_name (string):  How you want your model to be named in the output table\n        model:                A fit GridSearchCV object\n        X_test_data:          numpy array of X_test data\n        y_test_data:          numpy array of y_test data\n\n    Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model\n    '''\n\n    preds = model.best_estimator_.predict(X_test_data)\n\n    auc = roc_auc_score(y_test_data, preds)\n    accuracy = accuracy_score(y_test_data, preds)\n    precision = precision_score(y_test_data, preds)\n    recall = recall_score(y_test_data, preds)\n    f1 = f1_score(y_test_data, preds)\n\n    table = pd.DataFrame({'model': [model_name],\n                          'precision': [precision], \n                          'recall': [recall],\n                          'f1': [f1],\n                          'accuracy': [accuracy],\n                          'AUC': [auc]\n                         })\n  \n    return table","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:27:40.897075Z","iopub.execute_input":"2023-09-12T14:27:40.897457Z","iopub.status.idle":"2023-09-12T14:27:40.905489Z","shell.execute_reply.started":"2023-09-12T14:27:40.897424Z","shell.execute_reply":"2023-09-12T14:27:40.903650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the best performing model to predict on the test set.","metadata":{}},{"cell_type":"code","source":"# Get predictions on test data\nrf1_test_scores = get_scores('Random Forest CV 1', rf1, X_test, y_test)\nrf1_test_scores","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:27:44.702363Z","iopub.execute_input":"2023-09-12T14:27:44.703543Z","iopub.status.idle":"2023-09-12T14:27:44.818130Z","shell.execute_reply.started":"2023-09-12T14:27:44.703479Z","shell.execute_reply":"2023-09-12T14:27:44.816753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that the Random Forest model performed even better on the test data. This model seems to be a strong model and since the test set was only used for the Random Forest model, we can be confident that this model will perform accurately on new and unseen data.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\"><strong>Round 2</strong></span>\n\nThe very high evaluation scores might be indicative of overfitting or data leakage, or when the data used to train is not the expected data to be received once the model is deployed. It is very likely that the company will not have satisfaction scores for all employees. It is also possible for the `average_monthly_hours` column to be a source of data leakage. If the employees has already decided to quit or have already been identified as people to be fired, they make be working fewer hours than their peers.\n\nFor the second round of Contruct Stage, feature engineering will be incorporated to improve the models.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">Feature Engineering</span>\n\nDrop the `satisfaction_level` column and create a new feature called `overworked`. This feature is a binary variable and will identify the overworked employees based on their `average_monthly_hours`.","metadata":{}},{"cell_type":"code","source":"# Drop `satisfaction_level` and save resulting dataframe in new variable\ndf2 = df_enc.drop('satisfaction_level', axis=1)\n\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:28:18.505743Z","iopub.execute_input":"2023-09-12T14:28:18.506151Z","iopub.status.idle":"2023-09-12T14:28:18.528980Z","shell.execute_reply.started":"2023-09-12T14:28:18.506120Z","shell.execute_reply":"2023-09-12T14:28:18.527170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create `overworked` column. For now, it's identical to average monthly hours.\ndf2['overworked'] = df2['average_monthly_hours']\n\n# Inspect max, min and mean average_monthly_hours values\nprint('Max hours:', df2['overworked'].max())\nprint('Min hours:', df2['overworked'].min())\nprint('Mean hours:', df2['overworked'].mean())","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:28:22.671741Z","iopub.execute_input":"2023-09-12T14:28:22.672161Z","iopub.status.idle":"2023-09-12T14:28:22.680666Z","shell.execute_reply.started":"2023-09-12T14:28:22.672129Z","shell.execute_reply":"2023-09-12T14:28:22.679760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As identified earlier, 166.7 is approximately number of monthly hours for an employee who works 50 weeks annually, 5 days per week, and 8 hours per day. For this project, we will define overworked as someone who works more than 200 hours per month on average.","metadata":{}},{"cell_type":"code","source":"# Define `overworked` as working > 200 hrs/week\ndf2['overworked'] = (df2['overworked'] > 200).astype(int)\n\ndf2['overworked'].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:28:28.490175Z","iopub.execute_input":"2023-09-12T14:28:28.490512Z","iopub.status.idle":"2023-09-12T14:28:28.499517Z","shell.execute_reply.started":"2023-09-12T14:28:28.490490Z","shell.execute_reply":"2023-09-12T14:28:28.498647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the `average_monthly_hours` column\ndf2 = df2.drop('average_monthly_hours', axis=1)\n\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:28:31.961290Z","iopub.execute_input":"2023-09-12T14:28:31.962572Z","iopub.status.idle":"2023-09-12T14:28:31.983043Z","shell.execute_reply.started":"2023-09-12T14:28:31.962536Z","shell.execute_reply":"2023-09-12T14:28:31.982078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Isolate the target variable and the features. Then, split the data into training and testing sets.","metadata":{}},{"cell_type":"code","source":"# Isolate the outcome variable\ny = df2['left']\n\n# Select the features\nX = df2.drop('left', axis=1)\n\n# Create test data and stratify based on y\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:28:50.503453Z","iopub.execute_input":"2023-09-12T14:28:50.503830Z","iopub.status.idle":"2023-09-12T14:28:50.519131Z","shell.execute_reply.started":"2023-09-12T14:28:50.503799Z","shell.execute_reply":"2023-09-12T14:28:50.517417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">Decision Tree</span>\n\nConstruct a Decision Tree and with Cross Validation, through GridSearch, to exhaustively search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# Instantiate model\ntree = DecisionTreeClassifier(random_state=0)\n\n# Assign a dictionary of hyperparameters to search over\ncv_params = {'max_depth':[4, 6, 8, None],\n             'min_samples_leaf': [2, 5, 1],\n             'min_samples_split': [2, 4, 6]\n             }\n\n# Assign a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# Instantiate GridSearch\ntree2 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='f1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:28:56.004543Z","iopub.execute_input":"2023-09-12T14:28:56.004980Z","iopub.status.idle":"2023-09-12T14:28:56.013661Z","shell.execute_reply.started":"2023-09-12T14:28:56.004943Z","shell.execute_reply":"2023-09-12T14:28:56.011697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the Decision Tree model to the training data.\ntree2.fit(X_train, y_train) #Wall time: ~4s","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:29:05.506658Z","iopub.execute_input":"2023-09-12T14:29:05.507071Z","iopub.status.idle":"2023-09-12T14:29:09.329219Z","shell.execute_reply.started":"2023-09-12T14:29:05.507039Z","shell.execute_reply":"2023-09-12T14:29:09.327680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write pickle\nwrite_pickle(path, tree2, 'hr_tree2')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:29:38.125528Z","iopub.execute_input":"2023-09-12T14:29:38.126381Z","iopub.status.idle":"2023-09-12T14:29:38.131639Z","shell.execute_reply.started":"2023-09-12T14:29:38.126345Z","shell.execute_reply":"2023-09-12T14:29:38.130578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read pickle\ntree2 = read_pickle(path, 'hr_tree2')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:29:42.776536Z","iopub.execute_input":"2023-09-12T14:29:42.777003Z","iopub.status.idle":"2023-09-12T14:29:42.783566Z","shell.execute_reply.started":"2023-09-12T14:29:42.776965Z","shell.execute_reply":"2023-09-12T14:29:42.782253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best params\ntree2.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:29:48.012878Z","iopub.execute_input":"2023-09-12T14:29:48.013239Z","iopub.status.idle":"2023-09-12T14:29:48.020838Z","shell.execute_reply.started":"2023-09-12T14:29:48.013209Z","shell.execute_reply":"2023-09-12T14:29:48.019161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best F1 score on CV\ntree2.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:29:51.174534Z","iopub.execute_input":"2023-09-12T14:29:51.174961Z","iopub.status.idle":"2023-09-12T14:29:51.182473Z","shell.execute_reply.started":"2023-09-12T14:29:51.174924Z","shell.execute_reply":"2023-09-12T14:29:51.181352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model still performs very well despite using few features.","metadata":{}},{"cell_type":"code","source":"# Get all CV scores\ntree2_cv_results = make_results('Decision Tree CV 2', tree2, 'f1')\ntree_cv_results = pd.concat([tree_cv_results, tree2_cv_results], axis=0)\ntree_cv_results = tree_cv_results.reset_index(drop=True)\ntree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:29:54.214576Z","iopub.execute_input":"2023-09-12T14:29:54.214936Z","iopub.status.idle":"2023-09-12T14:29:54.234474Z","shell.execute_reply.started":"2023-09-12T14:29:54.214913Z","shell.execute_reply":"2023-09-12T14:29:54.233127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In case a row is to be dropped, input index number(s) in drop()\n#tree_cv_results = tree_cv_results.drop([5,6], axis=0).reset_index(drop=True)\n#tree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:30:34.891068Z","iopub.execute_input":"2023-09-12T14:30:34.891441Z","iopub.status.idle":"2023-09-12T14:30:34.895777Z","shell.execute_reply.started":"2023-09-12T14:30:34.891414Z","shell.execute_reply":"2023-09-12T14:30:34.894964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, all of the scores fell since fewer features were selected for this round; however, the scores are still very good.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">Random Forest</span>\n\nConstruct a Random Forest and with Cross Validation, through GridSearch, to exhaustively search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# Instantiate model\nrf = RandomForestClassifier(random_state=0)\n\n# Assign a dictionary of hyperparameters to search over\ncv_params = {'max_depth': [3,5, None], \n             'max_features': [1.0],\n             'max_samples': [0.7, 1.0],\n             'min_samples_leaf': [1,2,3],\n             'min_samples_split': [2,3,4],\n             'n_estimators': [300, 500],\n             }  \n\n# Assign a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# Instantiate GridSearch\nrf2 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='f1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:30:37.829094Z","iopub.execute_input":"2023-09-12T14:30:37.829481Z","iopub.status.idle":"2023-09-12T14:30:37.838942Z","shell.execute_reply.started":"2023-09-12T14:30:37.829450Z","shell.execute_reply":"2023-09-12T14:30:37.836244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the Random Forest model to the training data.\nrf2.fit(X_train, y_train) #Wall time ~23min","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:30:52.940696Z","iopub.execute_input":"2023-09-12T14:30:52.941070Z","iopub.status.idle":"2023-09-12T14:52:47.865087Z","shell.execute_reply.started":"2023-09-12T14:30:52.941040Z","shell.execute_reply":"2023-09-12T14:52:47.863274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write pickle\nwrite_pickle(path, rf2, 'hr_rf2')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:54:12.736498Z","iopub.execute_input":"2023-09-12T14:54:12.736943Z","iopub.status.idle":"2023-09-12T14:54:12.783230Z","shell.execute_reply.started":"2023-09-12T14:54:12.736909Z","shell.execute_reply":"2023-09-12T14:54:12.780858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in pickle\nrf2 = read_pickle(path, 'hr_rf2')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:54:18.749790Z","iopub.execute_input":"2023-09-12T14:54:18.750516Z","iopub.status.idle":"2023-09-12T14:54:18.792133Z","shell.execute_reply.started":"2023-09-12T14:54:18.750481Z","shell.execute_reply":"2023-09-12T14:54:18.790427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best params\nrf2.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:54:22.633693Z","iopub.execute_input":"2023-09-12T14:54:22.634076Z","iopub.status.idle":"2023-09-12T14:54:22.641798Z","shell.execute_reply.started":"2023-09-12T14:54:22.634047Z","shell.execute_reply":"2023-09-12T14:54:22.640427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best F1 score on CV\nrf2.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:54:27.185949Z","iopub.execute_input":"2023-09-12T14:54:27.186379Z","iopub.status.idle":"2023-09-12T14:54:27.193332Z","shell.execute_reply.started":"2023-09-12T14:54:27.186341Z","shell.execute_reply":"2023-09-12T14:54:27.191792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf2_cv_results = make_results('Random Forest CV 2', rf2, 'f1')\ntree_cv_results = pd.concat([tree_cv_results, rf2_cv_results], axis=0)\ntree_cv_results = tree_cv_results.reset_index(drop=True)\ntree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:54:32.452376Z","iopub.execute_input":"2023-09-12T14:54:32.452767Z","iopub.status.idle":"2023-09-12T14:54:32.474365Z","shell.execute_reply.started":"2023-09-12T14:54:32.452743Z","shell.execute_reply":"2023-09-12T14:54:32.473113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In case a row is to be dropped, input index number(s) in drop()\n#tree_cv_results = tree_cv_results.drop([5,6], axis=0).reset_index(drop=True)\n#tree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:54:36.992737Z","iopub.execute_input":"2023-09-12T14:54:36.993186Z","iopub.status.idle":"2023-09-12T14:54:36.998499Z","shell.execute_reply.started":"2023-09-12T14:54:36.993151Z","shell.execute_reply":"2023-09-12T14:54:36.997457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, the scores for the second round were a little lower than the first round.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">XGBoost</span>\n\nConstruct an XGBoost model and with Cross Validation, through GridSearch, to exhaustively search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# 1. Instantiate the XGBoost classifier\nxgb = XGBClassifier(objective='binary:logistic', random_state=0)\n\n# 2. Create a dictionary of hyperparameters to tune\ncv_params = {'learning_rate': [0.1,0.2,0.3],\n             'max_depth': [1,3,5,7],\n             'min_child_weight': [0.2,0.5,0.9,1,2,5],\n             'n_estimators': [30,50,100,200]\n             }\n\n# 3. Define a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# 4. Instantiate the GridSearchCV object\nxgb2 = GridSearchCV(xgb, cv_params, scoring=scoring, cv=4, refit='f1')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:54:41.218743Z","iopub.execute_input":"2023-09-12T14:54:41.219162Z","iopub.status.idle":"2023-09-12T14:54:41.226922Z","shell.execute_reply.started":"2023-09-12T14:54:41.219133Z","shell.execute_reply":"2023-09-12T14:54:41.225733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the XGBoost model to the training data.\nxgb2.fit(X_train, y_train) #Wall time: ~6min","metadata":{"execution":{"iopub.status.busy":"2023-09-12T14:55:04.510509Z","iopub.execute_input":"2023-09-12T14:55:04.510953Z","iopub.status.idle":"2023-09-12T15:00:36.196085Z","shell.execute_reply.started":"2023-09-12T14:55:04.510919Z","shell.execute_reply":"2023-09-12T15:00:36.194149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write pickle\nwrite_pickle(path, xgb2, 'hr_xgb2')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:01:57.581999Z","iopub.execute_input":"2023-09-12T15:01:57.582388Z","iopub.status.idle":"2023-09-12T15:01:57.591767Z","shell.execute_reply.started":"2023-09-12T15:01:57.582358Z","shell.execute_reply":"2023-09-12T15:01:57.590868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in pickle\nxgb2 = read_pickle(path, 'hr_xgb2')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:02:00.860421Z","iopub.execute_input":"2023-09-12T15:02:00.860821Z","iopub.status.idle":"2023-09-12T15:02:00.870046Z","shell.execute_reply.started":"2023-09-12T15:02:00.860778Z","shell.execute_reply":"2023-09-12T15:02:00.869203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best params\nxgb2.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:02:04.081951Z","iopub.execute_input":"2023-09-12T15:02:04.082318Z","iopub.status.idle":"2023-09-12T15:02:04.090939Z","shell.execute_reply.started":"2023-09-12T15:02:04.082289Z","shell.execute_reply":"2023-09-12T15:02:04.088951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check best F1 score on CV\nxgb2.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:02:08.964332Z","iopub.execute_input":"2023-09-12T15:02:08.964704Z","iopub.status.idle":"2023-09-12T15:02:08.972355Z","shell.execute_reply.started":"2023-09-12T15:02:08.964675Z","shell.execute_reply":"2023-09-12T15:02:08.971087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb2_cv_results = make_results('XGB CV 2', xgb2, 'f1')\ntree_cv_results = pd.concat([tree_cv_results, xgb2_cv_results], axis=0)\ntree_cv_results = tree_cv_results.reset_index(drop=True)\ntree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:02:14.278667Z","iopub.execute_input":"2023-09-12T15:02:14.279057Z","iopub.status.idle":"2023-09-12T15:02:14.299744Z","shell.execute_reply.started":"2023-09-12T15:02:14.279028Z","shell.execute_reply":"2023-09-12T15:02:14.298015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In case a row is to be dropped, input index number(s) in drop()\n#tree_cv_results = tree_cv_results.drop([5], axis=0).reset_index(drop=True)\n#tree_cv_results","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:02:11.616995Z","iopub.execute_input":"2023-09-12T15:02:11.617339Z","iopub.status.idle":"2023-09-12T15:02:11.622247Z","shell.execute_reply.started":"2023-09-12T15:02:11.617305Z","shell.execute_reply":"2023-09-12T15:02:11.620971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, the scores dropped but the scores indicate that the model is a strong model. It appears that the second round, XGBoost model outperforms Decision Tree and Random Forest.","metadata":{}},{"cell_type":"markdown","source":"Use the best performing model to predict on the test set.","metadata":{}},{"cell_type":"code","source":"# Get predictions on test data\nxgb2_test_scores = get_scores('XGB CV Test 2', xgb2, X_test, y_test)\nxgb2_test_scores","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:02:21.055056Z","iopub.execute_input":"2023-09-12T15:02:21.055447Z","iopub.status.idle":"2023-09-12T15:02:21.092515Z","shell.execute_reply.started":"2023-09-12T15:02:21.055416Z","shell.execute_reply":"2023-09-12T15:02:21.091096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the test scores, the second round XGBoost model appears to be a stable, well-performing model.\n\nFor this case study, the champion model to be chosen is the <strong>XGBoost</strong> model.\n\nAdditionally, plot a confusion matrix to visualize the performance of the model on test set.","metadata":{}},{"cell_type":"code","source":"# Generate array of values for confusion matrix\npreds = xgb2.best_estimator_.predict(X_test)\ncm = confusion_matrix(y_test, preds, labels=xgb2.classes_)\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                             display_labels=xgb2.classes_)\ndisp.plot(values_format='', cmap=plt.cm.YlGnBu)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:04:38.354914Z","iopub.execute_input":"2023-09-12T15:04:38.355265Z","iopub.status.idle":"2023-09-12T15:04:38.546943Z","shell.execute_reply.started":"2023-09-12T15:04:38.355239Z","shell.execute_reply":"2023-09-12T15:04:38.545942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model predicts a little more false positives than false negatives. This means that at the process of predicting employees that will leave, there is a higher probability that the model might incorrectly identify employees as leaving when they are actually not, than the probability of the model incorrectly identifying employees as staying when they are actually leaving.\n\nFor further analysis, inspect the features with the most impact on an employee's decision to stay or leave.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Feature Importance</span>\n\n\n### <span style=\"color:#808080\">Decision Tree</span>\n\nFor the decision tree model, examine the decision splits and the most important features.","metadata":{}},{"cell_type":"code","source":"# Plot the decision tree split\nplt.figure(figsize=(85,20))\nplot_tree(tree2.best_estimator_, max_depth=6, fontsize=14, feature_names=X.columns, \n          class_names={0:'stayed', 1:'left'}, filled=True);\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:05:58.469687Z","iopub.execute_input":"2023-09-12T15:05:58.470087Z","iopub.status.idle":"2023-09-12T15:06:02.830559Z","shell.execute_reply.started":"2023-09-12T15:05:58.470056Z","shell.execute_reply":"2023-09-12T15:06:02.828807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort features based on their importance using gini_importance\ntree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, \n                                 columns=['gini_importance'], \n                                 index=X.columns\n                                )\ntree2_importances = tree2_importances.sort_values(by='gini_importance', ascending=False)\n\n# Only extract the features with importances > 0\ntree2_importances = tree2_importances[tree2_importances['gini_importance'] != 0]\ntree2_importances","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:06:11.633947Z","iopub.execute_input":"2023-09-12T15:06:11.634348Z","iopub.status.idle":"2023-09-12T15:06:11.649422Z","shell.execute_reply.started":"2023-09-12T15:06:11.634319Z","shell.execute_reply":"2023-09-12T15:06:11.648344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a barplot to visualize the decision tree feature importances\nsns.barplot(data=tree2_importances, x=\"gini_importance\", y=tree2_importances.index, orient='h', color='#3373a1')\nplt.title(\"Decision Tree: Feature Importances for Employee Leaving\", fontsize=12)\nplt.ylabel(\"Feature\")\nplt.xlabel(\"Importance\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:12:14.737212Z","iopub.execute_input":"2023-09-12T15:12:14.737688Z","iopub.status.idle":"2023-09-12T15:12:15.002547Z","shell.execute_reply.started":"2023-09-12T15:12:14.737657Z","shell.execute_reply":"2023-09-12T15:12:15.000757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the diagram, the decision tree model predicts that the most important features are `last_evaluation`, `number_project`, `tenure`, and `overworked`, in order, in determining the outcome variable, `left`.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">Random Forest</span>\n\nNext, examine most important features for the Random Forest model.","metadata":{}},{"cell_type":"code","source":"# Get feature importances\nfeat_impt = rf2.best_estimator_.feature_importances_\n\n# Get indices of top 10 features\nind = np.argpartition(rf2.best_estimator_.feature_importances_, -10)[-10:]\n\n# Get column labels of top 10 features \nfeat = X.columns[ind]\n\n# Filter `feat_impt` to consist of top 10 feature importances\nfeat_impt = feat_impt[ind]\n\ny_df = pd.DataFrame({\"Feature\":feat,\"Importance\":feat_impt})\ny_sort_df = y_df.sort_values(\"Importance\")\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\ny_sort_df.plot(kind='barh',ax=ax1,x=\"Feature\",y=\"Importance\", color = \"#3373a1\")\n\nax1.set_title(\"Random Forest: Feature Importances for Employee Leaving\", fontsize=12)\nax1.set_ylabel(\"Feature\")\nax1.set_xlabel(\"Importance\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:12:19.629091Z","iopub.execute_input":"2023-09-12T15:12:19.629451Z","iopub.status.idle":"2023-09-12T15:12:19.926573Z","shell.execute_reply.started":"2023-09-12T15:12:19.629423Z","shell.execute_reply":"2023-09-12T15:12:19.924911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the diagram, the random forest model also predicts that the most important features are `last_evaluation`, `number_project`, `tenure`, and `overworked`, in order, in determining the outcome variable, `left`.","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color:#808080\">XGBoost</span>\n\nFinally, examine most important features for the Random Forest model.","metadata":{}},{"cell_type":"code","source":"# Get feature importances\nfeat_impt = xgb2.best_estimator_.feature_importances_\n\n# Get indices of top 10 features\nind = np.argpartition(xgb2.best_estimator_.feature_importances_, -10)[-10:]\n\n# Get column labels of top 10 features \nfeat = X.columns[ind]\n\n# Filter `feat_impt` to consist of top 10 feature importances\nfeat_impt = feat_impt[ind]\n\ny_df = pd.DataFrame({\"Feature\":feat,\"Importance\":feat_impt})\ny_sort_df = y_df.sort_values(\"Importance\")\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\ny_sort_df.plot(kind='barh',ax=ax1,x=\"Feature\",y=\"Importance\", color='#3373a1')\n\nax1.set_title(\"XGBoost: Feature Importances for Employee Leaving\", fontsize=12)\nax1.set_ylabel(\"Feature\")\nax1.set_xlabel(\"Importance\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:12:24.648231Z","iopub.execute_input":"2023-09-12T15:12:24.648639Z","iopub.status.idle":"2023-09-12T15:12:24.914350Z","shell.execute_reply.started":"2023-09-12T15:12:24.648590Z","shell.execute_reply":"2023-09-12T15:12:24.913232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The champion model also predicts the same top four most important features but different order. Based on the XGBoost model, the most important features are `number_project`, `overworked`, `tenure`, and `last_evaluation`, in order, in determining the outcome variable, `left`.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#35a455\"><strong>EXECUTE STAGE</strong></span>\n\n## <span style=\"color:#808080\">Results</span>","metadata":{}},{"cell_type":"markdown","source":"The logistic regression model achieved 86% precision, 93% recall, 90% f1-score, and accuracy of 82%. On the other hand, the tree-based modeling achieved 86% precision, 93% recall, 90% f1-score, and accuracy of 82%, with XGBoost model being chosen as the champion model.\n\nThe factors with the greatest impact on an employee's decision to leave are (1) the number of projects that employee is assigned to, (2) whether that employee is overworked or not, (3) tenure in the company, and (4) the most recent evaluation score.\n","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#808080\">Conclusions and Recommendations</span>","metadata":{}},{"cell_type":"markdown","source":"The results of this case study have shown that a huge portion of the company is overworked. To improve employee retention, this study recommends the following:\n\n* Restrict the number of projects that the employees get assigned to. Set the limit to 5 projects.\n* Re-evaluate project timelines and expectations, taking into account the amount of effort the employees can provide without overworking them. This can be done by assuming each employee can work for only 8 hours per day and has two weeks of vacation annually.\n* Consider hiring more people so the employees need not work overtime.\n* Continue and recalibrate the employee evaluation system to make sure those employees who contribute more or put in more effort have high evaluation scores.\n* Investigate if employees are paid accordingly when they work overtime. If not, inform employees about company policies regarding overtime pay and make these policies explicit. If they are paid accordingly for working overtime, anticipate the effects on employee satisfaction when they are getting limited overtime, thus, lower overall pay as well. The decrease in satisfaction due to lower overall pay can be mitigated by implementing a salary increase for all employees, or those with satisfactory to high evaluation scores.\n* Create an Award System that will reward employees upon reaching a certain number of years in the company.\n* Conduct further investigation why four-year tenured employees have very low satisfaction levels. Also, consider giving them incentives when they reach the four-year mark in the company.\n* Initiate company-wide and team-wide retrospectives and discussions to understand and address issues with company culture.\n\nTo further improve the results of this study, it is recommended to consider other factors that might affect an employee's decision to leave such as the distance between the company and the employee's residence, their salary growth in the past years, participation in activities that encourage work-life balance, and opportunities for growth in the company.","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}